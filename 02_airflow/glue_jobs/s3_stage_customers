import boto3
import psycopg2
from awsglue.utils import getResolvedOptions
import sys
from datetime import datetime, timedelta

# Fetch job parameters
args = getResolvedOptions(
    sys.argv, 
    ['REDSHIFT_IAM_ROLE', 'AWS_BUCKET_NAME', 'REDSHIFT_USER', 'REDSHIFT_PASSWORD', 'REDSHIFT_DBNAME', 'REDSHIFT_HOST', 'REDSHIFT_PORT']
)

# Extract parameters
redshift_iam_role = args['REDSHIFT_IAM_ROLE']
s3_bucket = args['AWS_BUCKET_NAME']
redshift_user = args['REDSHIFT_USER']
redshift_password = args['REDSHIFT_PASSWORD']
redshift_db = args['REDSHIFT_DBNAME']
redshift_host = args['REDSHIFT_HOST']
redshift_port = args['REDSHIFT_PORT']

# Function to determine the ETL batch date
def get_etl_batch_date():
    """Fetch the ETL batch date from Redshift."""
    connection = None
    cursor = None
    try:
        # Connect to Redshift using the provided connection details
        connection = psycopg2.connect(
            dbname=redshift_db,
            user=redshift_user,
            password=redshift_password,
            host=redshift_host,
            port=redshift_port
        )
        
        cursor = connection.cursor()
        etl_batch_date_query = "SELECT etl_batch_date FROM metadata.batch_control"
        cursor.execute(etl_batch_date_query)
        result = cursor.fetchone()
        
        if result is None:
            print("No ETL batch date found in the batch_control table.")
            return None
        
        return result[0]  # Return the first column of the result, which is the batch date
    except Exception as e:
        print(f"Error fetching batch date from Redshift: {e}")
        return None
    finally:
        if cursor:
            cursor.close()
        if connection:
            connection.close()

# Function to load data to Redshift
def load_data_to_redshift(table_name, s3_folder, etl_batch_date):
    connection = None
    try:
        connection = psycopg2.connect(
            dbname=redshift_db,
            user=redshift_user,
            password=redshift_password,
            host=redshift_host,
            port=redshift_port
        )
        cursor = connection.cursor()

        # Construct S3 file path
        s3_key = f"s3://{s3_bucket}/{s3_folder}/{etl_batch_date}/{table_name}.csv"

        # COPY command
        copy_command = f"""
        COPY devstage.{table_name} (
            CUSTOMERNUMBER, CUSTOMERNAME, CONTACTLASTNAME, CONTACTFIRSTNAME, PHONE, ADDRESSLINE1,
            ADDRESSLINE2, CITY, STATE, POSTALCODE, COUNTRY, SALESREPEMPLOYEENUMBER, CREDITLIMIT, CREATE_TIMESTAMP, UPDATE_TIMESTAMP
        )
        FROM '{s3_key}'
        IAM_ROLE '{redshift_iam_role}'
        FORMAT AS CSV
        DELIMITER ','
        QUOTE '"'
        IGNOREHEADER 1
        DATEFORMAT 'auto'
        TIMEFORMAT 'auto'
        ACCEPTINVCHARS AS '?'
        FILLRECORD
        IGNOREBLANKLINES
        TRIMBLANKS
        EMPTYASNULL;
        """

        cursor.execute(copy_command)
        connection.commit()
        cursor.close()
        print(f"Data successfully loaded into Redshift table devstage.{table_name} from {s3_key}")
    except Exception as e:
        print(f"Error loading data into Redshift: {e}")
    finally:
        if connection:
            connection.close()

# Main Execution
etl_batch_date = get_etl_batch_date()
if etl_batch_date:
    load_data_to_redshift('CUSTOMERS', 'CUSTOMERS', etl_batch_date)
else:
    print("ETL batch date not found. Exiting.")
